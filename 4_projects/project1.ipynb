{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU langchain-groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if \"GROQ_API_KEY\" not in os.environ:\n",
    "    os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"Enter your Groq API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate our model object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    # other params...\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I'm functioning properly, thank you for asking. I'm a large language model, so I don't have emotions or feelings like humans do, but I'm here to help answer any questions or provide information you might need. How can I assist you today?\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 53, 'prompt_tokens': 41, 'total_tokens': 94, 'completion_time': 0.070666667, 'prompt_time': 0.007701828, 'queue_time': 0.06578560600000001, 'total_time': 0.078368495}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_a56f6eea01', 'finish_reason': 'stop', 'logprobs': None}, id='run-b140fae6-e49d-4c24-9425-4a46c3b0e597-0', usage_metadata={'input_tokens': 41, 'output_tokens': 53, 'total_tokens': 94})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"Hello, how are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The translation of \"I love programming\" to French is:\\n\\n\"J\\'adore le programmation.\"', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 22, 'prompt_tokens': 55, 'total_tokens': 77, 'completion_time': 0.029333333, 'prompt_time': 0.011183663, 'queue_time': 2.9522059869999997, 'total_time': 0.040516996}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_a56f6eea01', 'finish_reason': 'stop', 'logprobs': None}, id='run-f45ceadd-900b-4733-b167-f81879118f1b-0', usage_metadata={'input_tokens': 55, 'output_tokens': 22, 'total_tokens': 77})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are a helpful assistant that translates English to French. Translate the user sentence.\",\n",
    "    ),\n",
    "    (\"human\", \"I love programming.\"),\n",
    "]\n",
    "ai_msg = llm.invoke(messages)\n",
    "ai_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", \"You are english to german language translator. Translate the user sentence.\"),\n",
    "    (\"user\", \"{input}\")],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply a and b.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "@tool\n",
    "def divide(a: int, b: int) -> float:\n",
    "    \"\"\"Divide a by b.\"\"\"\n",
    "    return a / b\n",
    "\n",
    "@tool\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Add a and b.\"\"\"\n",
    "    return a + b\n",
    "\n",
    "@tool\n",
    "def subtract(a: int, b: int) -> int:\n",
    "    \"\"\"Subtract b from a.\"\"\"    \n",
    "    return a - b\n",
    "\n",
    "@tool\n",
    "def floor_divide(a: int, b: int) -> int:\n",
    "    \"\"\"Divide a by b and round down.\"\"\"\n",
    "    return a // b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_h9g3', 'function': {'arguments': '{\"a\": 2, \"b\": 3}', 'name': 'add'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 448, 'total_tokens': 466, 'completion_time': 0.024, 'prompt_time': 0.014816846, 'queue_time': 0.048062519000000005, 'total_time': 0.038816846}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_a4265e44d5', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-7e46e430-7388-4fbc-949d-f36cc8f0e1d3-0', tool_calls=[{'name': 'add', 'args': {'a': 2, 'b': 3}, 'id': 'call_h9g3', 'type': 'tool_call'}], usage_metadata={'input_tokens': 448, 'output_tokens': 18, 'total_tokens': 466})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools = [multiply, divide, add, subtract, floor_divide]\n",
    "\n",
    "model_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "model_with_tools.invoke(\"add 2 and 3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Joke(setup='Why did the cat join a band?', punchline='Because it wanted to be the purr-cussionist!', rating=8)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Optional\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "# Pydantic\n",
    "class Joke(BaseModel):\n",
    "    \"\"\"Joke to tell user.\"\"\"\n",
    "\n",
    "    setup: str = Field(description=\"The setup of the joke\")\n",
    "    punchline: str = Field(description=\"The punchline to the joke\")\n",
    "    rating: Optional[int] = Field(\n",
    "        default=None, description=\"How funny the joke is, from 1 to 10\"\n",
    "    )\n",
    "\n",
    "\n",
    "structured_llm = llm.with_structured_output(Joke)\n",
    "\n",
    "structured_llm.invoke(\"Tell me a joke about cats\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TemperatureSensor(sensor_id='Mumbai_Temp', location='Mumbai', temperature=28.0, unit='Celsius', timestamp=datetime.datetime(2023, 3, 17, 0, 0), status='active')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional\n",
    "from datetime import datetime\n",
    "\n",
    "class TemperatureSensor(BaseModel):\n",
    "    sensor_id: str = Field(description=\"Unique identifier for the sensor\")\n",
    "    location: str = Field(description=\"Location of the sensor\")\n",
    "    temperature: float = Field(description=\"Current temperature reading from the sensor\")\n",
    "    unit: str = Field(default=\"Celsius\", description=\"Unit of the temperature reading\")\n",
    "    timestamp: datetime = Field(default_factory=datetime.now, description=\"Timestamp of the reading\")\n",
    "    status: Optional[str] = Field(default=\"active\", description=\"Status of the sensor (e.g., active, inactive, maintenance)\")\n",
    "\n",
    "structured_llm = llm.with_structured_output(TemperatureSensor)\n",
    "\n",
    "structured_llm.invoke(\"Based on weather condition of specific date and day, what would be the temperature of mumbai in Celsius?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bug Logger POC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional\n",
    "from datetime import datetime\n",
    "\n",
    "class TechnicalBugLog(BaseModel):\n",
    "    bug_id: str = Field(description=\"Unique identifier for the bug\")\n",
    "    description: str = Field(description=\"Description of the bug\")\n",
    "    severity: int = Field(description=\"Severity of the bug, from 1 to 10\")\n",
    "    steps_to_reproduce: str = Field(description=\"Steps to reproduce the bug\")\n",
    "    actual_behavior: str = Field(description=\"Actual behavior observed\")\n",
    "    expected_behavior: str = Field(description=\"Expected behavior\")\n",
    "    system_info: str = Field(description=\"System information where the bug was observed\")\n",
    "    timestamp: datetime = Field(default_factory=datetime.now, description=\"Timestamp of the bug report\")\n",
    "    status: Optional[str] = Field(default=\"open\", description=\"Status of the bug (e.g., open, closed, in progress)\")\n",
    "\n",
    "structured_llm = llm.with_structured_output(TechnicalBugLog)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"You are an AI assistant specialized in analyzing web session logs to help developers reproduce and understand bugs. When given a session recording log (consisting of timestamped user actions, network requests, and errors), analyze it carefully and produce:\n",
    "\n",
    "A clear, step-by-step reproduction sequence for the observed issue\n",
    "Identification of the error patterns and their potential root causes\n",
    "Technical context about the errors (e.g., API failures, JSON parsing issues)\n",
    "Relevant environment information extracted from the logs\n",
    "\n",
    "Focus on extracting:\n",
    "\n",
    "User navigation paths (URLs visited)\n",
    "User interactions (clicks, form inputs)\n",
    "Error messages and their patterns\n",
    "Network request failures\n",
    "The sequence and timing of events\n",
    "\n",
    "Format your response as a formal bug report with:\n",
    "\n",
    "Environment details\n",
    "Clear numbered reproduction steps\n",
    "Observed issues section with technical details\n",
    "Additional observations that might help developers understand the context\n",
    "If possible, identify whether the issue is likely client-side or server-side\n",
    "\n",
    "Present the information in a structured, technical manner suitable for engineering teams, but ensure it remains understandable to non-technical stakeholders who may need to prioritize the issues.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_prompt = \"\"\"This data was extracted from a Chrome extension that provides information about actions performed by the user and the elements: {data}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system_prompt),\n",
    "    (\"user\", human_prompt)],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
